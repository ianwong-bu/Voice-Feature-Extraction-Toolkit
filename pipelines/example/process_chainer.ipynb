{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: {'language': 'en: English', 'likelihood': 0.9994450211524963}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "def audio_request(input_file, url, return_json=True):\n",
    "    if not os.path.isfile(input_file):\n",
    "        print(f\"Error: File {input_file} does not exist.\")\n",
    "        return None\n",
    "    \n",
    "    with open(input_file, 'rb') as file:\n",
    "        files = {'audio': file}\n",
    "        try:\n",
    "            response = requests.post(url, files=files)\n",
    "            response.raise_for_status()\n",
    "            return response.json() if return_json else response.text\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return None\n",
    "\n",
    "def text_request(input_text, url):\n",
    "    try:\n",
    "        response = requests.post(url, json={'text': input_text})\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Usage example\n",
    "BASE_DIR = \"./standardized\"\n",
    "input_file = \"first_ten_Sample_HV_Clip.wav\"\n",
    "full_path = os.path.join(BASE_DIR, input_file)\n",
    "\n",
    "# Language identification from audio\n",
    "lang_url = \"http://localhost:8001/identify_language\"\n",
    "lang_result = audio_request(full_path, lang_url)\n",
    "print(f\"Detected language: {lang_result}\" if lang_result else \"Failed to detect language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASR result: NOW WE ARE RECORDING WE'RE GOING TO START OFF SOME BASIC DEMOCRATIC QUESTIONS WHAT IS YOUR CURRENT MYRTLE STATUS MARRIED NO YOU WRITE OR LEFT HANDED\n"
     ]
    }
   ],
   "source": [
    "# ASR\n",
    "asr_url = \"http://localhost:8001/transcribe\"\n",
    "asr_result = audio_request(full_path, asr_url, return_json=False)\n",
    "print(f\"ASR result: {asr_result}\" if asr_result else \"Failed to transcribe audio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK sentiment: {'sentiment_scores': {'compound': -0.296, 'neg': 0.081, 'neu': 0.919, 'pos': 0.0}}\n",
      "NLTK POS: {'tagged': [['NOW', 'IN'], ['WE', 'NNP'], ['ARE', 'NNP'], ['RECORDING', 'NNP'], ['WE', 'NNP'], [\"'RE\", 'VBD'], ['GOING', 'NNP'], ['TO', 'NNP'], ['START', 'NNP'], ['OFF', 'NNP'], ['SOME', 'NNP'], ['BASIC', 'NNP'], ['DEMOCRATIC', 'NNP'], ['QUESTIONS', 'NNP'], ['WHAT', 'NNP'], ['IS', 'VBZ'], ['YOUR', 'JJ'], ['CURRENT', 'NNP'], ['MYRTLE', 'NNP'], ['STATUS', 'NNP'], ['MARRIED', 'NNP'], ['NO', 'NNP'], ['YOU', 'NNP'], ['WRITE', 'NNP'], ['OR', 'NNP'], ['LEFT', 'NNP'], ['HANDED', 'NNP']]}\n",
      "Brunet index: {'brunet_index': 6.790982785097943, 'total_words': 26, 'unique_words': 25}\n",
      "Lemmatization: {'lemmas': ['now', 'we', 'be', 'record', \"WE'RE\", 'go', 'to', 'start', 'off', 'some', 'basic', 'democratic', 'question', 'what', 'be', 'your', 'current', 'MYRTLE', 'STATUS', 'MARRIED', 'no', 'you', 'write', 'or', 'LEFT', 'HANDED']}\n"
     ]
    }
   ],
   "source": [
    "if asr_result:\n",
    "    # NLTK sentiment\n",
    "    sentiment_url = \"http://localhost:8000/sentiment\"\n",
    "    sentiment = text_request(asr_result, sentiment_url)\n",
    "    if sentiment:\n",
    "        print(f\"NLTK sentiment: {sentiment}\" if sentiment else \"Failed to get sentiment\")\n",
    "\n",
    "    # NLTK POS tagging\n",
    "    pos_url = \"http://localhost:8000/pos_tag\"\n",
    "    pos = text_request(asr_result, pos_url)\n",
    "    if pos:\n",
    "        print(f\"NLTK POS: {pos}\" if pos else \"Failed to get POS tags\")\n",
    "\n",
    "    # BRUNet index - NLP container\n",
    "    brunet_url = \"http://localhost:8002/brunet_index\"\n",
    "    brunet = text_request(asr_result, brunet_url)\n",
    "    if brunet:\n",
    "        print(f\"Brunet index: {brunet}\" if brunet else \"Failed to get Brunet index\")\n",
    "        \n",
    "    # Lemmatization - Spacy\n",
    "    lemmatization_url = \"http://localhost:8003/lemmatize\"\n",
    "    lemmatization = text_request(asr_result, lemmatization_url)\n",
    "    if lemmatization:\n",
    "        print(f\"Lemmatization: {lemmatization}\" if lemmatization else \"Failed to get lemmatization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
