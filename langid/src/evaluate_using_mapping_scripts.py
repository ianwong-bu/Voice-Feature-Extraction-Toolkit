# -*- coding: utf-8 -*-
"""Evaluate using mapping scripts

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zydGkFd414PapPQ0ywfHgGGNZ63dRUi7
"""

from dataclasses import dataclass, asdict
from typing import Any
import datetime
import sys
from pathlib import Path
import json
import time
import importlib

import datasets
import transformers
from transformers import AutoModel, Trainer, AutoModelForAudioClassification, TrainingArguments, AutoFeatureExtractor
import evaluate
from torch.utils.data import DataLoader
import torch
from tqdm.auto import trange, tqdm
import numpy as np
import matplotlib.pyplot as plt
import iso639

import global_id_utils
import eval_report

timestamp_str = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

model_id = sys.argv[1]
dataset_id = sys.argv[2]

output_dir = Path("/output") / (model_id.replace('/', '_') + " on " + dataset_id.replace('/', '_'))
mappings_dir = Path("/app/mappings")

"""# Load model"""

model = AutoModelForAudioClassification.from_pretrained(model_id)
feature_extractor = AutoFeatureExtractor.from_pretrained(model_id)

"""# Load data"""

dataset_config_name = sys.argv[3]
dataset_split = sys.argv[4]

dataset = datasets.load_dataset(dataset_id, dataset_config_name, split=dataset_split, trust_remote_code=True)

dataset_config = eval_report.DatasetConfig(
    dataset_id,
    dataset_config_name,
    dataset_split,
    f'datasets.load_dataset(dataset_id, "{dataset_config_name}", split="{dataset_split}", trust_remote_code=True)'
)

"""## Load mappings"""

model_mappings_dir = mappings_dir / "models" / model_id
dataset_mappings_dir = mappings_dir / "datasets" / dataset_id

def load_mapping(path: Path):
  with open(path, "r") as in_file:
    mapping = json.load(in_file)
  mapping_integer_keys = {int(k): v for k, v in mapping.items()}
  return mapping_integer_keys

model_id_to_global_id = load_mapping(model_mappings_dir / "model_id_to_global_id.json")
global_id_to_model_id = load_mapping(model_mappings_dir / "global_id_to_model_id.json")

dataset_id_to_global_id = load_mapping(dataset_mappings_dir / "dataset_id_to_global_id.json")
global_id_to_dataset_id = load_mapping(dataset_mappings_dir / "global_id_to_dataset_id.json")

"""# Preprocess data"""

import importlib
import importlib.util

def import_file(module_name, file_path):
  spec = importlib.util.spec_from_file_location(module_name, file_path)
  module = importlib.util.module_from_spec(spec)
  spec.loader.exec_module(module)

  return module

dataset_preprocess_dataset = import_file("preprocess_dataset", "/app/mapping_scripts/datasets/" + dataset_id + "/preprocess_dataset.py")
model_preprocess_dataset = import_file("preprocess_dataset", "/app/mapping_scripts/models/" + model_id + "/preprocess_dataset.py")

standard_dataset = dataset_preprocess_dataset.to_standard_dataset(dataset, dataset_id_to_global_id)

model_dataset = model_preprocess_dataset.to_model_dataset(standard_dataset, model, feature_extractor, global_id_to_model_id)

accuracy = evaluate.load("accuracy")

def compute_metrics(pred):
    logits, labels = pred
    predictions = np.argmax(logits, axis=-1)
    acc = accuracy.compute(predictions=predictions, references=labels)
    return acc

model_dataset = model_dataset.select_columns(feature_extractor.model_input_names + ["label"])

"""# Make inferences"""

args = TrainingArguments(
    output_dir=output_dir,
    per_device_eval_batch_size=1,
    logging_steps=25,
)

trainer = Trainer(
    args=args,
    model=model,
    eval_dataset=model_dataset,
    compute_metrics=compute_metrics,
)

prediction_output = trainer.predict(model_dataset)

"""# Evaluate"""

import eval_report

import importlib
importlib.reload(eval_report)

comments = ""
eval_report.make_evaluation_outputs(prediction_output, output_dir, model_id_to_global_id, model.config.id2label, dataset_id_to_global_id, model_id, dataset_config, comments)