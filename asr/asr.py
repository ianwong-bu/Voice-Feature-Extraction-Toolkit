# -*- coding: utf-8 -*-
"""ASR Evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tbdwxr_dQO-JdVhnLKACq1URBwDlGXMe
"""

from pathlib import Path
import json
import sys

import datasets
import transformers
import evaluate

output_base_dir = Path("/output")

model_id = sys.argv[1]
dataset_id = sys.argv[2]
dataset_config_name = sys.argv[3]
dataset_split = sys.argv[4]

output_dir = output_base_dir / (model_id.replace("/", "_") + " on " + dataset_id.replace("/", "_"))

# TODO: Dynamically call correct/user defined function for dataset
def load_dataset(dataset_id, name, split, other_args=None):
  dataset = (datasets
    .load_dataset(dataset_id, name=name, split=split, **(other_args or {}))
    .cast_column("audio", datasets.Audio(sampling_rate=16_000))
    .select_columns(("audio", "transcription")))

  return dataset

dataset = load_dataset(dataset_id, dataset_config_name, dataset_split)

transcriber = transformers.pipeline("automatic-speech-recognition", model=model_id)

predictions = transcriber(dataset["audio"])

predictions = [pred["text"] for pred in predictions]

wer = evaluate.load("wer")

wer_metric = wer.compute(predictions=predictions,  references=dataset["transcription"])
wer_metric

output_dir.mkdir(exist_ok=True)
with open(output_dir / "metadata.json", "w") as out_file:
  json.dump({
      "model_id": model_id,
      "dataset_id": dataset_id,
      "dataset_config_name": dataset_config_name,
      "dataset_split": dataset_split,
  }, out_file)
with open(output_dir / "predictions.json", "w") as out_file:
  json.dump({
      "predictions": predictions,
      "reference": dataset["transcription"]
  }, out_file)

with open(output_dir  / "metrics.json", "w") as out_file:
  json.dump({
      "wer": wer_metric,
  }, out_file)